{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26033a93",
   "metadata": {},
   "source": [
    "# BGG Board Game Recommendation Engine\n",
    "\n",
    "<img src=\"nb-assets/shelf.jpeg\">\n",
    "<p style=\"text-align:center\"> [Pictured: my (main) game shelf] </p>\n",
    "\n",
    "Let me describe a common scenario in my life: I find myself at a mall as a result of marital obligation, and I'm fortunate enough to locate the _game store_. Gazing upon the glorious bounty on the shelves, I reach for an article of particular interest and begin reading the description...\n",
    "\n",
    "45 minutes later, I still haven't bought anything. Why? I have no idea which of these grossly overpriced items will actually be worth the investment (seriously, board games are expensive). So to save my marriage (I lied - we're at the mall because I _wanted_ to look at board games. Sue me) I decided to design a recommendation system so I can stop buying games by accident, and start looking for the games I will actually like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e8b3f",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "\n",
    "<img src=\"nb-assets/catan.jpg\">\n",
    "\n",
    "The first step of this project is data collection. Anyone who is into board games will be familiar with the source I've chosen: [Board Game Geek](https://boardgamegeek.com/). It's a very popular website for documenting, discussing, rating and reviewing all sorts of boardgames and tabletop games. Each game has detailed statistics reflecting user submitted ratings, genre and category information, a \"complexity score\" metric, and more - lots of potential features to feed an ML model. I'm going to be collecting data from the top 20,000 games as listed on their [rankings page](https://boardgamegeek.com/browse/boardgame/page/100?sort=rank). But first, I need to collect the list of games and links to the individual pages. Here we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e010c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b329a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # To make HTTP requests\n",
    "import re # For pattern matching some of the results\n",
    "import json # For parsing (certain) results\n",
    "import time # To implement scraping delay\n",
    "import pandas as pd # To view the data in DataFrame format\n",
    "from bs4 import BeautifulSoup # To parse the HTML from request objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6eb1f5",
   "metadata": {},
   "source": [
    "## Targeting Data - CSS Selectors and HTML Tags\n",
    "\n",
    "<img src=\"nb-assets/devtools.png\" height=\"400\" width=\"680\">\n",
    "\n",
    "<p style=\"text-align:center\"> \"Developer tools are your friend.\" <br> - Some guy on StackOverflow</p>\n",
    "\n",
    "This section represents the trial and error approach of finding the right HTML tags and CSS selectors to access the page info we need. I'm using [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the HTML from the request objects. \n",
    "\n",
    "### Note on robots.txt:\n",
    "\n",
    "Most websites have a robots.txt file that can be found by adding `/robots.txt` to the base URL. This is where the site's administrators designate their preferred webscraping/webcrawling activity. I checked for BGG, and they list a crawl delay of 5s, so that's what I have adhered to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab90905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate the URL of the target page\n",
    "url = 'https://boardgamegeek.com/browse/boardgame/page/1?sort=rank'\n",
    "content_1 = requests.get(url)\n",
    "# Initialize the BeautifulSoup object\n",
    "rankings = BeautifulSoup(content_1.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db8a84d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gloomhaven'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name\n",
    "rankings.select('tr#row_')[0].find('a', class_='primary').string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2e0ac4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year\n",
    "rankings.select('tr#row_')[0].find('span').string[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "975cce5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/boardgame/174430/gloomhaven'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rel filepath (part of the URL that leads to the individual game's page)\n",
    "rankings.select('tr#row_')[0].find('a', class_='primary').get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0f5f37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'174430'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ID\n",
    "rankings.select('tr#row_')[0].find('a', class_='primary').get('href').split('/')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e6291c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51469'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average user rating\n",
    "rankings.select('tr#row_')[0].find_all('td', class_='collection_bggrating')[1].string.strip()\n",
    "# Number of votes\n",
    "rankings.select('tr#row_')[0].find_all('td', class_='collection_bggrating')[2].string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6e7a5",
   "metadata": {},
   "source": [
    "---\n",
    "Here I set up my first function to scrape rankings pages. I set it up to save the data in a csv every 10 pages (every 1000 records). It did not go as planned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94257d",
   "metadata": {},
   "source": [
    "## Getting past the login wall\n",
    "\n",
    "<img src=\"nb-assets/loginwall.png\" height=\"240\" width=\"480\">\n",
    "<p style=\"text-align:center\">Well that's not right.</p>\n",
    "\n",
    "After running a first past through the top 200 rankings pages, I noticed something pretty weird - my DataFrames were all the same size! It seems like to view rankings pages past 20 (the first 2000 results), you need to be logged in. I will skip over the hours of Google and StackOverflow and get to the point: to log in to this site with only the requests module, I needed two things: \n",
    " - My login credentials formatted in JSON with correct keyword names\n",
    " - The URL of the API endpoint to make a POST request \n",
    " \n",
    "Fortunately, I found the answer through, you guessed it: Chrome Developer tools. By recording network activity and logging in, I was able to find the required formatting for my login details and the correct URL to send my POST request to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "bf8d5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, initialize a requests Session object that can store\n",
    "# the authentication cookie and access the remaining pages to scrape.\n",
    "s = requests.Session()\n",
    "\n",
    "headers = {\"Accept-Language\":\"en-US,en;q=0.9\",\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\"}\n",
    "data = {\"credentials\": {\"username\": \"\", \"password\": \"\"}}\n",
    "url = \"https://boardgamegeek.com/login/api/v1\"\n",
    "\n",
    "r = s.post(url, json=data, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5f376e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "count = 1 # Starting page\n",
    "url = 'https://boardgamegeek.com/browse/boardgame/page/' # Base URL\n",
    "params = {'sort':'rank'} # Sort method\n",
    "\n",
    "# Initialize an empty list to store game data\n",
    "game_data = []\n",
    "\n",
    "# Create the main scraping loop \n",
    "while count <= 200:\n",
    "    # Make the request\n",
    "    content = s.get(url+str(count), params=params)\n",
    "    # Error handling: if response code is not 200 'OK', we move\n",
    "    # to the next page and print a note to the console\n",
    "    if content.status_code != 200:\n",
    "        print(f'Failed on page {count}')\n",
    "        continue\n",
    "        \n",
    "    # Parse the data from our response object\n",
    "    page = BeautifulSoup(content.content, 'html.parser')\n",
    "    \n",
    "    # Begin collecting the targeted data\n",
    "    for idx, game in enumerate(page.select('tr#row_')):\n",
    "        \n",
    "        # Error handling: if any of these selections throw an error, we\n",
    "        # print a note to the console and move on to the next game\n",
    "        try:\n",
    "            id_ = game.find('a', class_='primary').get('href').split('/')[2]\n",
    "            name = game.find('a', class_='primary').string\n",
    "            year = game.find('span').string[1:5]\n",
    "            filepath = game.find('a', class_='primary').get('href')\n",
    "            rating = game.find_all('td', class_='collection_bggrating')[1].string.strip()\n",
    "            votecount = game.find_all('td', class_='collection_bggrating')[2].string.strip()\n",
    "        except:\n",
    "            print(f'error with game {idx} on page {count}')\n",
    "            continue\n",
    "            \n",
    "        # If everything worked, we update game_data\n",
    "        data = [id_, name, year, filepath, rating, votecount]\n",
    "        game_data.append(data)\n",
    "            \n",
    "    # Failsafe: save a copy of the data every 10 pages\n",
    "    if count % 10 == 0:\n",
    "        pd.DataFrame(game_data).to_csv(f'data{count}.csv', index=False)\n",
    "    \n",
    "    # Sleep 5s per the crawl delay on the site's robots.txt\n",
    "    time.sleep(5)\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635243e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only two entries were invalid - not bad! \n",
    "# I also ended up with a duplicate, so I'll drop it\n",
    "df.columns = ['id', 'title', 'year', 'url', 'user_score', 'votes']\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "98752028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/ranks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7294a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://boardgamegeek.com/boardgame/174430/gloomhaven/'\n",
    "url_credits = 'https://boardgamegeek.com/boardgame/174430/gloomhaven/credits'\n",
    "\n",
    "game_page = BeautifulSoup(s.get(url).content, 'html.parser')\n",
    "credits = BeautifulSoup(s.get(url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aaacaf",
   "metadata": {},
   "source": [
    "## Scraping the Game Details Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d89c7b",
   "metadata": {},
   "source": [
    "<img src=\"nb-assets/page2.png\" height=\"400\" width=\"600\">\n",
    "\n",
    "Looks easy to get, but turns out not so much. Rather than being directly in the HTML, this info is actually populated via JavaScript. A JSON file is served when the page is loaded - this JSON file has the data I'm looking for. \n",
    "\n",
    "After some poking around, I ended up scraping the whole set twice - the first time, using an exposed API (URL redacted from this notebook) found using Chrome Developer Tools (duh). The second time I scraped the individual pages to get the leftover data.\n",
    "\n",
    "And again, per the crawl-delay stated on the site's [robots.txt](https://boardgamegeek.com/robots.txt), I scraped at a delay of 5s (I timed it - it took about .5s to process each request. Hence the 4.5s sleep calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First pass, taking advantage of the API \n",
    "\n",
    "details = []\n",
    "for count, id_ in enumerate(df.id.values[16792:]):\n",
    "    url = f'[API URL]'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    # Error handling - prints an error message each time data can't be collected\n",
    "    try:\n",
    "        # Max # of players\n",
    "        players = data['item']['maxplayers']\n",
    "        # Min # of players (solo yes/no)\n",
    "        solo = data['item']['minplayers'] == '1'\n",
    "        # Min-age (categorize: kid, teen, adult)\n",
    "        age = data['item']['minage']\n",
    "        # Minimum playtime\n",
    "        playtime = data['item']['minplaytime']\n",
    "        # Description\n",
    "        description = BeautifulSoup(data['item']['description'], 'html.parser').text.strip()\n",
    "        # Designer(s)\n",
    "        designers = [designer['name'] for designer in data['item']['links']['boardgamedesigner']]\n",
    "        # Publisher (main)\n",
    "        publisher = data['item']['links']['boardgamepublisher'][0]['name']\n",
    "        # Number of awards\n",
    "        awards = data['item']['linkcounts']['boardgamehonor']\n",
    "        # Categories\n",
    "        categories = [category['name'] for category in data['item']['links']['boardgamecategory']]\n",
    "        # Game mechanics ## credits\n",
    "        mechanics = [mechanic['name'] for mechanic in data['item']['links']['boardgamemechanic']]\n",
    "        # Genres\n",
    "        genres = [subgenre['name'] for subgenre in data['item']['links']['boardgamesubdomain']]\n",
    "    except:\n",
    "        print(f'error on index {count}')\n",
    "        continue\n",
    "    \n",
    "    # Update the list\n",
    "    details.append([id_, players, solo, age, playtime, description, designers, publisher, awards, categories, mechanics])\n",
    "    \n",
    "    # Failsafe - save the data every 100 entries\n",
    "    if count % 100 == 0:\n",
    "        pd.DataFrame(details, columns = ['id', 'players', 'solo', 'age', 'playtime', 'description', 'designers', 'publisher', 'awards', 'categories', 'mechanics']).to_csv(f'data/details{count}.csv')\n",
    "    # Be a good citizen!\n",
    "    time.sleep(4.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d835ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass - scraping the pages directly and getting\n",
    "# remaining data from the JSON files served on each page\n",
    "\n",
    "# New list to hold details\n",
    "details_again = []\n",
    "# Base url\n",
    "base_url = 'https://boardgamegeek.com'\n",
    "count = 0\n",
    "for game in df.url.values:\n",
    "    # Added error handling for potential network issues\n",
    "    try:\n",
    "        url = f'{base_url}{game}'\n",
    "        response = requests.get(url)\n",
    "        game_page = BeautifulSoup(response.content, 'html.parser') \n",
    "        match = re.search(r'GEEK.geekitemPreload = ({.*})', game_page.find_all('script')[2].string)\n",
    "        data = json.loads(match.groups()[0])\n",
    "    except:\n",
    "        print(f'Error in requests on index {count}')\n",
    "        time.sleep(4.5)\n",
    "        continue\n",
    "        \n",
    "    # Changed the error handling pattern here: I opted to include NaN entries \n",
    "    # when taking in this data after I noticed a very high rate of errors late\n",
    "    # in the scraping process.\n",
    "    \n",
    "    try:\n",
    "        # Best # players\n",
    "        best_players = data['item']['polls']['userplayers']['best'][0]['max']\n",
    "    except:\n",
    "        best_players = None\n",
    "    try:\n",
    "        # complexity score\n",
    "        weight = data['item']['stats']['avgweight']\n",
    "    except:\n",
    "        weight = None\n",
    "    try:\n",
    "        # Min # of players \n",
    "        min_players = data['item']['minplayers']\n",
    "    except:\n",
    "        min_players = None\n",
    "        # Minimum playtime\n",
    "    try:\n",
    "        max_playtime = data['item']['maxplaytime']\n",
    "    except:\n",
    "        max_playtime = None\n",
    "        \n",
    "    # Update the list\n",
    "    details_again.append([df.iloc[count].id, best_players, min_players, max_playtime, weight])\n",
    "    \n",
    "    # Again, save the data every 100 entries as a failsafe\n",
    "    if count % 100 == 0:\n",
    "        current_data = pd.DataFrame(details_again, columns = ['id', 'best_players', 'min_players', 'max_playtime', 'weight'])\n",
    "        current_data.to_csv(f'data/extra_details{count}.csv', index=False)\n",
    "    count += 1\n",
    "    if count > 19998:\n",
    "        break\n",
    "    # Be a good citizen!\n",
    "    time.sleep(4.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
